# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x1iNvW12CyJiwevbI-2Tgg-DAQmTDvaT

This command installs the necessary Kaggle library so that your notebook can interact with the Kaggle website.
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install kaggle

"""Upload Your Kaggle API Key"""

from google.colab import files
files.upload()

"""Configure the Kaggle API Key"""

import os

# Create the .kaggle directory if it doesn't exist
!mkdir -p ~/.kaggle

# Move the kaggle.json file to the .kaggle directory
!mv kaggle.json ~/.kaggle/

# Set the permissions for the kaggle.json file
!chmod 600 ~/.kaggle/kaggle.json

"""Download the Competition Dataset"""

!kaggle competitions download -c facial-keypoints-detection

"""Unzip the Main Dataset File"""

!unzip facial-keypoints-detection.zip

""" Unzip the Training Data"""

!unzip training.zip

"""Unzip the Test Data"""

!unzip test.zip

"""Import All Necessary Libraries"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt

"""Load the CSV Files into Pandas DataFrames"""

train_df = pd.read_csv('training.csv')
test_df = pd.read_csv('test.csv')

"""Handle Missing Data"""

train_df.dropna(inplace=True)

""" Convert Data for the Model"""

X = np.array([np.fromstring(image, dtype=int, sep=' ') for image in train_df['Image']]).reshape(-1, 96, 96, 1) / 255.0
y = train_df.drop('Image', axis=1).values

"""Split Data into Training and Validation Sets"""

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

"""Define the Neural Network Architecture"""

model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(96, 96, 1)),
    BatchNormalization(),
    MaxPooling2D(pool_size=(2, 2)),

    Conv2D(64, (3,3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D(pool_size=(2, 2)),

    Conv2D(128, (3,3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D(pool_size=(2, 2)),

    Flatten(),

    Dense(512, activation='relu'),
    Dropout(0.2),
    Dense(30)
])

"""Compile the Model"""

model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])

"""Train the Model"""

early_stopping = EarlyStopping(patience=10, restore_best_weights=True)
history = model.fit(X_train, y_train, epochs=100, batch_size=128, validation_data=(X_val, y_val), callbacks=[early_stopping])

"""Visualizing the Learning Process (The Graphs)"""

# Plot the training and validation loss
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Plot the training and validation Mean Absolute Error (MAE)
plt.subplot(1, 2, 2)
plt.plot(history.history['mae'], label='Training MAE')
plt.plot(history.history['val_mae'], label='Validation MAE')
plt.title('Mean Absolute Error Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('MAE')
plt.legend()

plt.show()

"""Visually Checking the Predictions (The Most Intuitive Result)"""

# Preprocess the test data's images just like you did for the training data
X_test = np.array([np.fromstring(image, dtype=int, sep=' ') for image in test_df['Image']]).reshape(-1, 96, 96, 1) / 255.0

# Use the trained model to make predictions on the test images
predictions = model.predict(X_test)

# Clip the predictions to be within the valid range [0, 96]
predictions = np.clip(predictions, 0, 96)

# ===================================================================
# VISUALLY CHECKING THE PREDICTIONS
# ===================================================================

# --- Function to plot keypoints on an image ---
def plot_keypoints(img, points):
    plt.imshow(img.reshape(96, 96), cmap='gray')
    plt.scatter(points[0::2], points[1::2], marker='x', s=20, c='red')
    plt.show()

# --- Display a few random images with their predicted keypoints ---
import random

# Select 3 random image indices from the test set
num_samples_to_show = 3
random_indices = random.sample(range(len(X_test)), num_samples_to_show)

for i in random_indices:
    print(f"Showing prediction for test image #{i}")
    # We are now using the single 'predictions' variable we created above
    plot_keypoints(X_test[i], predictions[i])

"""The Final Submission File"""

# --- Step 1: Make predictions on the test set (if you haven't already) ---
# Ensure the test images are loaded and preprocessed
X_test = np.array([np.fromstring(image, dtype=int, sep=' ') for image in test_df['Image']]).reshape(-1, 96, 96, 1) / 255.0

# Get the model's predictions for all test images
predictions = model.predict(X_test)


# --- Step 2: Generate the submission file using the IdLookupTable ---

# Load the lookup table and the sample submission file
lookup_df = pd.read_csv('IdLookupTable.csv')
submission_df = pd.read_csv('SampleSubmission.csv')

# Create a mapping from feature names to the column index
feature_names = list(train_df.drop('Image', axis=1).columns)
feature_indices = {feature: i for i, feature in enumerate(feature_names)}

# Create an empty list to store the final predicted locations
locations = []

# Loop through each row of the lookup table to find the corresponding prediction
for index, row in lookup_df.iterrows():
    image_id = row['ImageId']
    feature_name = row['FeatureName']
    feature_index = feature_indices[feature_name]

    # We are now using the single, already-clipped 'predictions' variable
    predicted_value = predictions[image_id - 1][feature_index]
    locations.append(predicted_value)

# Assign the list of predicted locations to the 'Location' column
submission_df['Location'] = locations

# Save the final DataFrame to a CSV file
submission_df.to_csv('submission.csv', index=False)

print("Successfully created submission.csv!")